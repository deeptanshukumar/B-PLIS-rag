{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b6b6677",
   "metadata": {},
   "source": [
    "# B-PLIS-RAG: Experiment Notebook\n",
    "\n",
    "This notebook demonstrates the B-PLIS-RAG system for legal domain RAG with ReFT and activation steering.\n",
    "\n",
    "## Contents\n",
    "1. Setup and Installation\n",
    "2. Data Loading\n",
    "3. Model Loading\n",
    "4. ReFT Intervention Training\n",
    "5. Activation Steering\n",
    "6. RAG Pipeline Demo\n",
    "7. Evaluation\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-org/B-PLIS-rag/blob/main/notebooks/experiment.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9218ea",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c4e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Colab: Clone repo and install dependencies\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if running in Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/your-org/B-PLIS-rag.git\n",
    "    %cd B-PLIS-rag\n",
    "    !pip install -q -r requirements.txt\n",
    "else:\n",
    "    # Local development - add parent to path\n",
    "    sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeef885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eb063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import B-PLIS-RAG modules\n",
    "from src.config import get_config, setup_environment\n",
    "from src.model_loader import load_model, get_model_info\n",
    "from src.reft import ReFTIntervention, ReFTTrainer, ReFTHook, verify_intervention\n",
    "from src.activation_steering import ActivationSteering, compute_faithfulness_metrics\n",
    "from src.data_handler import LegalBenchRAG, DataHandler\n",
    "from src.retriever import FAISSRetriever\n",
    "from src.rag_pipeline import RAGPipeline\n",
    "from src.evaluator import Evaluator\n",
    "\n",
    "# Setup environment (seeds, etc.)\n",
    "setup_environment()\n",
    "print(\"Modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0de8650",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "Load the LegalBench-RAG dataset with legal documents and benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ce2911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data handler\n",
    "data_handler = LegalBenchRAG()\n",
    "\n",
    "# Download dataset (skip if already downloaded)\n",
    "# Uncomment the next line to download\n",
    "# data_handler.download()\n",
    "\n",
    "print(\"Data handler initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4646887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus (use small subset for demo)\n",
    "documents = data_handler.load_corpus(\n",
    "    corpus_types=[\"contractnli\"],  # Start with one corpus\n",
    "    max_docs_per_type=50  # Limit for demo\n",
    ")\n",
    "\n",
    "# Show statistics\n",
    "stats = data_handler.get_corpus_stats()\n",
    "print(f\"\\nLoaded {stats['total_documents']} documents\")\n",
    "for corpus, info in stats['corpus_types'].items():\n",
    "    print(f\"  {corpus}: {info['num_documents']} docs, avg length: {info['avg_doc_length']:.0f} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad9d0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load benchmarks\n",
    "benchmarks = data_handler.load_benchmarks()\n",
    "\n",
    "print(f\"\\nLoaded {len(benchmarks)} benchmarks:\")\n",
    "for name, examples in benchmarks.items():\n",
    "    print(f\"  {name}: {len(examples)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5b3567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conflict examples for training\n",
    "conflict_examples = data_handler.create_conflict_examples(num_examples=20)\n",
    "\n",
    "print(f\"\\nCreated {len(conflict_examples)} conflict examples\")\n",
    "\n",
    "# Show a sample\n",
    "if conflict_examples:\n",
    "    sample = conflict_examples[0]\n",
    "    print(f\"\\nSample conflict example:\")\n",
    "    print(f\"  Query: {sample.query[:100]}...\")\n",
    "    print(f\"  Context: {sample.context[:200]}...\")\n",
    "    print(f\"  Answer: {sample.context_answer[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1bd349",
   "metadata": {},
   "source": [
    "## 3. Model Loading\n",
    "\n",
    "Load the T5-base model with optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a259b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load T5-base model\n",
    "model, tokenizer = load_model(\n",
    "    model_name=\"t5-base\",\n",
    "    device=str(device),\n",
    "    freeze_params=True,  # Freeze for efficiency\n",
    ")\n",
    "\n",
    "# Show model info\n",
    "info = get_model_info(model)\n",
    "print(f\"\\nModel: {info['name']}\")\n",
    "print(f\"Hidden size: {info['hidden_size']}\")\n",
    "print(f\"Decoder layers: {info['num_decoder_layers']}\")\n",
    "print(f\"Total params: {info['total_params']:,}\")\n",
    "print(f\"Trainable params: {info['trainable_params']:,}\")\n",
    "print(f\"Device: {info['device']}\")\n",
    "print(f\"Dtype: {info['dtype']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e06350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic generation\n",
    "test_prompt = \"What is a contract?\"\n",
    "\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Q: {test_prompt}\")\n",
    "print(f\"A: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac03eead",
   "metadata": {},
   "source": [
    "## 4. ReFT Intervention Training\n",
    "\n",
    "Train a low-dimensional latent intervention to steer the model toward context-faithful generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62830273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ReFT intervention\n",
    "intervention = ReFTIntervention(\n",
    "    hidden_size=model.config.d_model,  # 768 for t5-base\n",
    "    intervention_dim=16,  # Low-dimensional latent\n",
    "    init_std=0.02,\n",
    ")\n",
    "intervention = intervention.to(device)\n",
    "\n",
    "print(f\"\\nReFT Intervention:\")\n",
    "print(f\"  Intervention dim: {intervention.intervention_dim}\")\n",
    "print(f\"  Hidden size: {intervention.hidden_size}\")\n",
    "print(f\"  Parameters: {intervention.num_parameters()}\")\n",
    "print(f\"  % of model: {100 * intervention.num_parameters() / info['total_params']:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef0dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training examples\n",
    "training_examples = [\n",
    "    {\n",
    "        \"query\": ex.query,\n",
    "        \"context\": ex.context,\n",
    "        \"answer\": ex.context_answer,\n",
    "    }\n",
    "    for ex in conflict_examples[:10]  # Use subset for demo\n",
    "]\n",
    "\n",
    "# If no real data, use synthetic examples\n",
    "if not training_examples:\n",
    "    training_examples = [\n",
    "        {\n",
    "            \"query\": \"What is confidential information?\",\n",
    "            \"context\": \"Confidential Information means any non-public information disclosed by one party.\",\n",
    "            \"answer\": \"Confidential Information means any non-public information disclosed by one party.\",\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"What is the term of this agreement?\",\n",
    "            \"context\": \"This Agreement shall be effective for three (3) years from the Effective Date.\",\n",
    "            \"answer\": \"Three years from the Effective Date.\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "print(f\"Training on {len(training_examples)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bb21c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = ReFTTrainer(\n",
    "    model=model,\n",
    "    intervention=intervention,\n",
    "    tokenizer=tokenizer,\n",
    "    target_layer=6,  # Mid-layer for balanced intervention\n",
    "    learning_rate=1e-2,\n",
    "    num_steps=50,  # Steps per example\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(\"Trainer created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf63444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the intervention\n",
    "print(\"Training ReFT intervention...\")\n",
    "results = trainer.train(training_examples, epochs=1, verbose=True)\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"  Final avg loss: {results['avg_loss']:.4f}\")\n",
    "print(f\"  Z norm: {results['z_norm']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66aef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the intervention changes outputs\n",
    "verification = verify_intervention(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    intervention=intervention,\n",
    "    target_layer=6,\n",
    "    test_prompt=\"What is a breach of contract?\",\n",
    ")\n",
    "\n",
    "print(\"\\nVerification Results:\")\n",
    "print(f\"  Outputs differ: {verification['outputs_differ']}\")\n",
    "print(f\"  Verification passed: {verification['verification_passed']}\")\n",
    "print(f\"\\n  Zero-z output: {verification['text_zero_z'][:100]}...\")\n",
    "print(f\"  Non-zero-z output: {verification['text_nonzero_z'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a72a5ab",
   "metadata": {},
   "source": [
    "## 5. Activation Steering\n",
    "\n",
    "Compute and apply activation steering vectors for context focus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7bebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create activation steerer\n",
    "steerer = ActivationSteering(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    layer=6,  # Same layer as ReFT\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(\"Activation steerer created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ea068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare prompts for steering vector computation\n",
    "positive_prompts = [  # With context\n",
    "    f\"Use context: {ex['context']}\\n\\nQuestion: {ex['query']}\\n\\nAnswer:\"\n",
    "    for ex in training_examples[:5]\n",
    "]\n",
    "\n",
    "negative_prompts = [  # Without context\n",
    "    f\"Question: {ex['query']}\\n\\nAnswer:\"\n",
    "    for ex in training_examples[:5]\n",
    "]\n",
    "\n",
    "print(f\"Computing steering vector from {len(positive_prompts)} prompt pairs...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fa2556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute steering vector\n",
    "steering_vector = steerer.compute_steering_vector(\n",
    "    positive_prompts=positive_prompts,\n",
    "    negative_prompts=negative_prompts,\n",
    "    normalize=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nSteering vector computed!\")\n",
    "print(f\"  Shape: {steering_vector.shape}\")\n",
    "print(f\"  Norm: {steering_vector.norm().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3228c47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test steering effect\n",
    "test_context = \"The agreement terminates after 24 months from signing.\"\n",
    "test_query = \"What is the duration of the agreement?\"\n",
    "test_prompt = f\"Use context: {test_context}\\n\\nQuestion: {test_query}\\n\\nAnswer:\"\n",
    "\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Without steering\n",
    "with torch.no_grad():\n",
    "    outputs_no_steer = model.generate(**inputs, max_new_tokens=50)\n",
    "text_no_steer = tokenizer.decode(outputs_no_steer[0], skip_special_tokens=True)\n",
    "\n",
    "# With steering\n",
    "with steerer.apply(multiplier=2.0):\n",
    "    with torch.no_grad():\n",
    "        outputs_steer = model.generate(**inputs, max_new_tokens=50)\n",
    "text_steer = tokenizer.decode(outputs_steer[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Context: {test_context}\")\n",
    "print(f\"Question: {test_query}\")\n",
    "print(f\"\\nWithout steering: {text_no_steer}\")\n",
    "print(f\"With steering: {text_steer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3eca43",
   "metadata": {},
   "source": [
    "## 6. RAG Pipeline Demo\n",
    "\n",
    "Put it all together with the complete RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cb69b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever and index documents\n",
    "retriever = FAISSRetriever(\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    device=str(device) if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "# Collect all documents\n",
    "all_docs = []\n",
    "for docs in documents.values():\n",
    "    all_docs.extend(docs)\n",
    "\n",
    "if all_docs:\n",
    "    print(f\"Indexing {len(all_docs)} documents...\")\n",
    "    retriever.index_documents(all_docs)\n",
    "    print(\"Indexing complete!\")\n",
    "else:\n",
    "    print(\"No documents to index. Using synthetic data for demo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6d183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full RAG pipeline\n",
    "pipeline = RAGPipeline(\n",
    "    model_name=\"t5-base\",\n",
    "    use_reft=True,\n",
    "    use_steering=True,\n",
    "    reft_layer=6,\n",
    "    reft_dim=16,\n",
    "    steering_layer=6,\n",
    "    steering_multiplier=2.0,\n",
    ")\n",
    "\n",
    "# Transfer our trained components\n",
    "pipeline.reft_intervention = intervention\n",
    "pipeline.steerer = steerer\n",
    "pipeline.retriever = retriever\n",
    "\n",
    "print(\"RAG pipeline ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ce6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is confidential information?\",\n",
    "    \"Define breach of contract.\",\n",
    "    \"What are the termination clauses?\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    response = pipeline.query(query, top_k=3)\n",
    "    \n",
    "    print(f\"\\nAnswer: {response.answer}\")\n",
    "    print(f\"\\nSources:\")\n",
    "    for i, src in enumerate(response.sources[:2], 1):\n",
    "        print(f\"  {i}. [{src['source']}] Score: {src['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931ce103",
   "metadata": {},
   "source": [
    "## 7. Evaluation\n",
    "\n",
    "Evaluate the pipeline on benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2569f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluator\n",
    "evaluator = Evaluator(pipeline, data_handler)\n",
    "\n",
    "print(\"Evaluator ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda6ab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on a benchmark (use small subset for demo)\n",
    "benchmark_name = list(benchmarks.keys())[0] if benchmarks else \"contractnli\"\n",
    "\n",
    "print(f\"Evaluating on {benchmark_name}...\")\n",
    "metrics = evaluator.evaluate_benchmark(\n",
    "    benchmark_name=benchmark_name,\n",
    "    max_examples=10,  # Small subset for demo\n",
    ")\n",
    "\n",
    "print(f\"\\n{metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf65e50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with baseline (no steering)\n",
    "print(\"\\nComparison: Baseline vs Steered\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Baseline\n",
    "pipeline.use_reft = False\n",
    "pipeline.use_steering = False\n",
    "baseline_response = pipeline.query(\"What is confidential information?\", top_k=3)\n",
    "print(f\"Baseline answer: {baseline_response.answer}\")\n",
    "\n",
    "# Steered\n",
    "pipeline.use_reft = True\n",
    "pipeline.use_steering = True\n",
    "steered_response = pipeline.query(\"What is confidential information?\", top_k=3)\n",
    "print(f\"Steered answer: {steered_response.answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6b8e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained intervention\n",
    "save_path = \"checkpoints/reft_experiment.pt\"\n",
    "import os\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "trainer.save(save_path)\n",
    "print(f\"Intervention saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a4822c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Data Loading**: Loading LegalBench-RAG corpus and benchmarks\n",
    "2. **Model Setup**: T5-base with frozen parameters\n",
    "3. **ReFT Training**: Low-dimensional latent interventions (~0.01% of model params)\n",
    "4. **Activation Steering**: Computing and applying steering vectors\n",
    "5. **RAG Pipeline**: Complete retrieval-augmented generation\n",
    "6. **Evaluation**: Character-level precision/recall metrics\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "- ReFT interventions can steer generation with minimal parameters\n",
    "- Activation steering provides runtime control over context focus\n",
    "- Combined approach (ReFT + steering) enhances faithfulness\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Train on full dataset (500+ examples)\n",
    "- Ablate over layers and intervention dimensions\n",
    "- Evaluate on all LegalBench-RAG benchmarks\n",
    "- Test bilingual capabilities (English + Hindi)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
