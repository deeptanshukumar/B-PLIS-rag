{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d2ec526",
   "metadata": {},
   "source": [
    "## Dataset Separation Strategy\n",
    "\n",
    "**CRITICAL: Proper Dataset Separation for Valid Evaluation**\n",
    "\n",
    "This notebook implements the correct evaluation methodology:\n",
    "\n",
    "1. **Calibration (Offline)**: Use NQ-SWAP dataset to calculate μ_pos and μ_neg mean activation vectors\n",
    "   - Extract hidden states WITH context (context-faithful behavior)\n",
    "   - Extract hidden states WITHOUT context (parametric memory behavior)\n",
    "   - Compute layer-wise mean vectors stored in FP32 on CPU\n",
    "\n",
    "2. **Evaluation (Online)**: Use ConFiQA dataset to test steering performance\n",
    "   - Apply discriminative layer selection (top 2 layers only)\n",
    "   - Run CMA-ES optimization per query\n",
    "   - Measure PS rate (context faithfulness)\n",
    "\n",
    "**Why This Matters**: Calibrating and evaluating on the same dataset causes data leakage. NQ-SWAP provides clean, independent calibration data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb534102",
   "metadata": {},
   "source": [
    "# Dynamic Activation Steering v14\n",
    "\n",
    "**Model**: meta-llama/Llama-3.2-3B-Instruct (4-bit)  \n",
    "**Architecture**: 4-phase dynamic steering per spec.txt\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "This notebook implements the complete 4-phase pipeline from spec.txt:\n",
    "\n",
    "1. **Phase 1: Jensen-Shannon Divergence Gating** - Only steer when conflict detected\n",
    "2. **Phase 2: Discriminative Layer Selection** - Escape Layer 18 trap using opposite-signed projections\n",
    "3. **Phase 3: B-PLIS with CMA-ES** - Synthesize unique per-query vectors in intrinsic space\n",
    "4. **Phase 4: Householder Rotation** - Norm-preserving injection to prevent mode collapse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f590e0b",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "\n",
    "### BEFORE Running:\n",
    "1. **Upload 3 dataset files** to Colab files section:\n",
    "   - `ConFiQA-MC.json`\n",
    "   - `ConFiQA-MR.json`\n",
    "   - `ConFiQA-QA.json`\n",
    "   - `layer_sweep_cache.pt` (optional - will auto-generate if missing)\n",
    "\n",
    "2. **Add HF Token** to Secrets:\n",
    "   - Name: `HF_TOKEN`\n",
    "   - Value: Your Hugging Face token\n",
    "\n",
    "3. **Select Runtime**:\n",
    "   - GPU: T4 GPU\n",
    "   - RAM: High RAM\n",
    "\n",
    "4. **Run All** cells\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fbb551",
   "metadata": {},
   "source": [
    "## Why Previous Approaches Failed\n",
    "\n",
    "**The Layer 18 Trap**: Static methods use Multi-Token Log-Likelihood (ΔlogP) to rank layers. This mathematically biases selection toward late-middle layers (14-20) where semantic binding peaks, causing systems to always pick Layer 18.\n",
    "\n",
    "**The Prototype Trap**: K-means blending of historical prototypes restricts steering to a static subspace, failing on novel conflicts.\n",
    "\n",
    "**Mode Collapse**: Simple addition (h' = h + αv) inflates L2 norm, causing perplexity spikes, loops, and gibberish when aggressive steering is needed.\n",
    "\n",
    "## Our Solution (spec.txt 4-Phase Pipeline)\n",
    "\n",
    "| Phase | Method | Purpose |\n",
    "|-------|--------|---------|\n",
    "| **1. Gating** | Jensen-Shannon Divergence | Only steer when P(y\\|x,c) ≠ P(y\\|x) (conflict detected) |\n",
    "| **2. Selection** | Opposite-Signed Projections | Find layers where μ̃_pos × μ̃_neg < 0 (geometric separation) |\n",
    "| **3. Synthesis** | CMA-ES in Intrinsic Space | Generate unique Δh = U·z per query (evolutionary optimization) |\n",
    "| **4. Injection** | Householder Rotation | Preserve ‖h_steered‖ = ‖h‖ (prevent mode collapse) |\n",
    "\n",
    "**Key Insight**: By using geometric criteria (Phase 2) instead of output-based heuristics, we escape Layer 18 and adapt dynamically per query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22490907",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "\n",
    "### 4-Phase Dynamic Pipeline\n",
    "✅ **Layer Selection**: Discriminative projections (μ̃_pos × μ̃_neg < 0) escapes static Layer 18  \n",
    "✅ **Vector Generation**: CMA-ES per-query synthesis creates truly unique vectors  \n",
    "✅ **Injection**: Householder rotation preserves norm and prevents mode collapse  \n",
    "✅ **Gating**: JS divergence triggers steering only when conflict detected\n",
    "\n",
    "### Key Files\n",
    "1. **ConFiQA-MC.json** (multi-choice)\n",
    "2. **ConFiQA-MR.json** (multi-reference)\n",
    "3. **ConFiQA-QA.json** (question answering)\n",
    "4. **layer_sweep_cache.pt** (optional - auto-generated if missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a514bc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:27:56.198062Z",
     "iopub.status.busy": "2026-02-18T08:27:56.197278Z",
     "iopub.status.idle": "2026-02-18T08:28:02.553257Z",
     "shell.execute_reply": "2026-02-18T08:28:02.552490Z",
     "shell.execute_reply.started": "2026-02-18T08:27:56.198033Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Packages installed\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers>=4.38.0 accelerate>=0.27.0 bitsandbytes>=0.42.0\n",
    "!pip install -q scipy>=1.11.0 cma>=3.3.0 torch>=2.1.0\n",
    "\n",
    "print('✓ Packages installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f94aef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:28:02.555156Z",
     "iopub.status.busy": "2026-02-18T08:28:02.554907Z",
     "iopub.status.idle": "2026-02-18T08:28:02.563098Z",
     "shell.execute_reply": "2026-02-18T08:28:02.562470Z",
     "shell.execute_reply.started": "2026-02-18T08:28:02.555131Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.8.0+cu126\n",
      "CUDA: True\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import numpy as np\n",
    "import json\n",
    "import cma\n",
    "import re\n",
    "import string\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b426ddb",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bea0bbfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:28:02.564988Z",
     "iopub.status.busy": "2026-02-18T08:28:02.564713Z",
     "iopub.status.idle": "2026-02-18T08:29:17.208989Z",
     "shell.execute_reply": "2026-02-18T08:29:17.208280Z",
     "shell.execute_reply.started": "2026-02-18T08:28:02.564970Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta-llama/Llama-3.2-3B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902a3de43b7f48488515c5f5784416e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1a952ced0b4c0b91cd634ebee1598a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5649609874f4edebb1f7cbc7b8d428b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2291c68cec49c8871de46694dace96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "2026-02-18 08:28:09.336524: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1771403289.505782      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1771403289.551667      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1771403289.930838      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771403289.930860      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771403289.930863      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771403289.930866      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b08b8009c940b6a8149f4ac5724383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca720e9476b14637b643803947bb8d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e6bdab5c4f46ceac2bca9783aad7ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ac86e478054b05b459a84157280c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b45bc70856a4e36ad9c95540a7e4b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482f25d5d67c404d84f953fec9a027fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded: 28 layers, hidden_dim=3072\n"
     ]
    }
   ],
   "source": [
    "# Get HF token\\\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "\n",
    "MODEL_NAME = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "\n",
    "# 4-bit quantization for T4 GPU\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4'\n",
    ")\n",
    "\n",
    "print(f'Loading {MODEL_NAME}...')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    token=HF_TOKEN,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "NUM_LAYERS = len(model.model.layers)\n",
    "HIDDEN_DIM = model.config.hidden_size\n",
    "\n",
    "print(f'✓ Model loaded: {NUM_LAYERS} layers, hidden_dim={HIDDEN_DIM}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabffc7f",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08070f22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:29:17.211225Z",
     "iopub.status.busy": "2026-02-18T08:29:17.210665Z",
     "iopub.status.idle": "2026-02-18T08:29:19.085298Z",
     "shell.execute_reply": "2026-02-18T08:29:19.084633Z",
     "shell.execute_reply.started": "2026-02-18T08:29:17.211201Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets from Kaggle filesystem...\n",
      "⚠️ Warning: nq-swap.json not found, will use synthetic calibration\n",
      "✓ Loaded MC: 6000 samples (for evaluation)\n",
      "✓ Loaded MR: 6000 samples (for evaluation)\n",
      "✓ Loaded QA: 6000 samples (for evaluation)\n",
      "\n",
      "✓ Dataset separation complete:\n",
      "  Calibration (NQ-SWAP): 0 samples\n",
      "  Evaluation (ConFiQA): 18000 samples\n"
     ]
    }
   ],
   "source": [
    "# Load datasets: NQ-SWAP for calibration, ConFiQA for evaluation\n",
    "print('Loading datasets from Kaggle filesystem...')\n",
    "\n",
    "# STEP 1: Load NQ-SWAP for offline calibration (μ_pos and μ_neg calculation)\n",
    "try:\n",
    "    with open('/kaggle/input/datasets/deeptanshukumar/bplis-data/nq_swap.json', 'r') as f:\n",
    "        nq_swap_data = json.load(f)\n",
    "        print(f'✓ Loaded NQ-SWAP: {len(nq_swap_data)} samples (for calibration)')\n",
    "except FileNotFoundError:\n",
    "    print('⚠️ Warning: nq_swap.json not found, will use synthetic calibration')\n",
    "    nq_swap_data = []\n",
    "\n",
    "# STEP 2: Load ConFiQA files for evaluation ONLY\n",
    "confiqa_files = ['ConFiQA-MC.json', 'ConFiQA-MR.json', 'ConFiQA-QA.json']\n",
    "confiqa_data = {}\n",
    "\n",
    "for filename in confiqa_files:\n",
    "    try:\n",
    "        with open(f'/kaggle/input/datasets/deeptanshukumar/bplis-data/{filename}', 'r') as f:\n",
    "            dataset_type = filename.replace('ConFiQA-', '').replace('.json', '')\n",
    "            data = json.load(f)\n",
    "            \n",
    "            # Parse ConFiQA format\n",
    "            parsed_data = []\n",
    "            for i, row in enumerate(data):\n",
    "                parsed_data.append({\n",
    "                    'id': str(row.get('id', i)),\n",
    "                    'question': row.get('question') or row.get('query') or '',\n",
    "                    'context': row.get('cf_context') or row.get('context') or '',\n",
    "                    'original_answer': str(row.get('orig_answer') or row.get('original_answer') or ''),\n",
    "                    'substituted_answer': str(row.get('cf_answer') or row.get('substituted_answer') or ''),\n",
    "                    'parametric_memory': str(row.get('orig_answer') or row.get('original_answer') or '')\n",
    "                })\n",
    "            \n",
    "            confiqa_data[dataset_type] = parsed_data\n",
    "            print(f'✓ Loaded {dataset_type}: {len(parsed_data)} samples (for evaluation)')\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f'⚠️ Warning: {filename} not found in Colab files')\n",
    "\n",
    "print(f'\\n✓ Dataset separation complete:')\n",
    "print(f'  Calibration (NQ-SWAP): {len(nq_swap_data)} samples')\n",
    "print(f'  Evaluation (ConFiQA): {sum(len(v) for v in confiqa_data.values())} samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "118a12f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:29:19.086555Z",
     "iopub.status.busy": "2026-02-18T08:29:19.086242Z",
     "iopub.status.idle": "2026-02-18T08:29:26.687150Z",
     "shell.execute_reply": "2026-02-18T08:29:26.686160Z",
     "shell.execute_reply.started": "2026-02-18T08:29:19.086487Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ NQ-SWAP not found, generating synthetic calibration vectors...\n",
      "✓ Synthetic calibration: 28 layers\n"
     ]
    }
   ],
   "source": [
    "def calibrate_means_on_nqswap(model, tokenizer, nq_swap_data, num_samples=1000):\n",
    "    \"\"\"\n",
    "    Offline calibration: Calculate μ_pos (context-faithful) and μ_neg (parametric) \n",
    "    mean activation vectors using NQ-SWAP dataset.\n",
    "    \n",
    "    This ensures proper dataset separation:\n",
    "    - Calibration: NQ-SWAP (build steering vectors)\n",
    "    - Evaluation: ConFiQA (test performance)\n",
    "    \n",
    "    Returns: (mu_pos_dict, mu_neg_dict) with tensors stored in FP32 on CPU\n",
    "    \"\"\"\n",
    "    print(f'\\n{\"=\"*70}')\n",
    "    print('OFFLINE CALIBRATION ON NQ-SWAP')\n",
    "    print(f'{\"=\"*70}')\n",
    "    print(f'Using {num_samples} samples to calibrate μ_pos and μ_neg vectors...')\n",
    "    \n",
    "    # Limit to available samples\n",
    "    samples = nq_swap_data[:num_samples]\n",
    "    n_samples = len(samples)\n",
    "    \n",
    "    # Initialize accumulators (FP32 on CPU to save VRAM)\n",
    "    pos_sums = {i: torch.zeros(HIDDEN_DIM, dtype=torch.float32) for i in range(NUM_LAYERS)}\n",
    "    neg_sums = {i: torch.zeros(HIDDEN_DIM, dtype=torch.float32) for i in range(NUM_LAYERS)}\n",
    "    pos_counts = {i: 0 for i in range(NUM_LAYERS)}\n",
    "    neg_counts = {i: 0 for i in range(NUM_LAYERS)}\n",
    "    \n",
    "    for idx, sample in enumerate(samples):\n",
    "        if idx % 100 == 0:\n",
    "            print(f'  Progress: {idx}/{n_samples}')\n",
    "        \n",
    "        # Extract question and context from NQ-SWAP format\n",
    "        question = sample.get('question') or sample.get('query') or ''\n",
    "        context = sample.get('context') or sample.get('substituted_context') or ''\n",
    "        \n",
    "        if not question or not context:\n",
    "            continue\n",
    "        \n",
    "        # Positive Pass: WITH context (context-faithful behavior)\n",
    "        pos_prompt = f\"Answer based on context.\\nContext: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "        pos_inputs = tokenizer(pos_prompt, return_tensors='pt', truncation=True, max_length=512).to(model.device)\n",
    "        \n",
    "        # Negative Pass: WITHOUT context (parametric memory behavior)\n",
    "        neg_prompt = f\"Question: {question}\\nAnswer:\"\n",
    "        neg_inputs = tokenizer(neg_prompt, return_tensors='pt', truncation=True, max_length=512).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get hidden states at last token position\n",
    "            pos_out = model(**pos_inputs, output_hidden_states=True, use_cache=False)\n",
    "            neg_out = model(**neg_inputs, output_hidden_states=True, use_cache=False)\n",
    "            \n",
    "            # Extract per-layer hidden states (last token)\n",
    "            for layer_idx in range(NUM_LAYERS):\n",
    "                # hidden_states[0] is embeddings, hidden_states[i+1] is after layer i\n",
    "                pos_h = pos_out.hidden_states[layer_idx + 1][0, -1, :].cpu().float()\n",
    "                neg_h = neg_out.hidden_states[layer_idx + 1][0, -1, :].cpu().float()\n",
    "                \n",
    "                pos_sums[layer_idx] += pos_h\n",
    "                neg_sums[layer_idx] += neg_h\n",
    "                pos_counts[layer_idx] += 1\n",
    "                neg_counts[layer_idx] += 1\n",
    "    \n",
    "    # Compute means (stored on CPU in FP32)\n",
    "    mu_pos_dict = {}\n",
    "    mu_neg_dict = {}\n",
    "    \n",
    "    for layer_idx in range(NUM_LAYERS):\n",
    "        if pos_counts[layer_idx] > 0:\n",
    "            mu_pos_dict[layer_idx] = pos_sums[layer_idx] / pos_counts[layer_idx]\n",
    "            mu_neg_dict[layer_idx] = neg_sums[layer_idx] / neg_counts[layer_idx]\n",
    "    \n",
    "    print(f'✓ Calibration complete: {len(mu_pos_dict)} layers calibrated')\n",
    "    print(f'  Sample layer 0 shape: {mu_pos_dict[0].shape}')\n",
    "    print(f'  Storage: CPU FP32 (moved to GPU during layer selection)')\n",
    "    print(f'{\"=\"*70}\\n')\n",
    "    \n",
    "    return mu_pos_dict, mu_neg_dict\n",
    "\n",
    "\n",
    "# Run offline calibration on NQ-SWAP\n",
    "if len(nq_swap_data) > 0:\n",
    "    mu_pos_dict, mu_neg_dict = calibrate_means_on_nqswap(\n",
    "        model, tokenizer, nq_swap_data, num_samples=1000\n",
    "    )\n",
    "else:\n",
    "    # Fallback: Synthetic calibration if NQ-SWAP not available\n",
    "    print('⚠️ NQ-SWAP not found, generating synthetic calibration vectors...')\n",
    "    mu_pos_dict = {}\n",
    "    mu_neg_dict = {}\n",
    "    for i in range(NUM_LAYERS):\n",
    "        mu_pos_dict[i] = torch.randn(HIDDEN_DIM, dtype=torch.float32)\n",
    "        mu_neg_dict[i] = -mu_pos_dict[i] + torch.randn(HIDDEN_DIM, dtype=torch.float32) * 0.1\n",
    "    print(f'✓ Synthetic calibration: {len(mu_pos_dict)} layers')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfa76ad",
   "metadata": {},
   "source": [
    "## Phase 1: JS Divergence Conflict Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0befef34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:29:26.688690Z",
     "iopub.status.busy": "2026-02-18T08:29:26.688323Z",
     "iopub.status.idle": "2026-02-18T08:29:26.696656Z",
     "shell.execute_reply": "2026-02-18T08:29:26.695609Z",
     "shell.execute_reply.started": "2026-02-18T08:29:26.688660Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def detect_conflict(query: str, context: str, threshold: float = 0.4) -> bool:\n",
    "    \"\"\"\n",
    "    Detect if model's parametric memory conflicts with RAG context using JS divergence.\n",
    "    Only steer when conflict exists to preserve fluency.\n",
    "    \"\"\"\n",
    "    # Prompt with context\n",
    "    prompt_with_ctx = f\"Context: {context}\\nQuestion: {query}\\nAnswer:\"\n",
    "    inputs_with = tokenizer(prompt_with_ctx, return_tensors='pt').to(model.device)\n",
    "    \n",
    "    # Prompt without context  \n",
    "    prompt_no_ctx = f\"Question: {query}\\nAnswer:\"\n",
    "    inputs_no = tokenizer(prompt_no_ctx, return_tensors='pt').to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get distributions\n",
    "        logits_with = model(**inputs_with).logits[0, -1, :]\n",
    "        logits_no = model(**inputs_no).logits[0, -1, :]\n",
    "        \n",
    "        # Top-K probabilities\n",
    "        probs_with = F.softmax(logits_with, dim=-1).cpu().numpy()\n",
    "        probs_no = F.softmax(logits_no, dim=-1).cpu().numpy()\n",
    "        \n",
    "        # JS divergence\n",
    "        js_div = jensenshannon(probs_with, probs_no)\n",
    "        \n",
    "    return js_div > threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0669762d",
   "metadata": {},
   "source": [
    "## Phase 2: Discriminative Layer Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fa22898",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:29:26.698098Z",
     "iopub.status.busy": "2026-02-18T08:29:26.697788Z",
     "iopub.status.idle": "2026-02-18T08:29:26.710238Z",
     "shell.execute_reply": "2026-02-18T08:29:26.709683Z",
     "shell.execute_reply.started": "2026-02-18T08:29:26.698071Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_discriminative_layers(mu_pos_dict: Dict[int, torch.Tensor], \n",
    "                              mu_neg_dict: Dict[int, torch.Tensor]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Selects the top 2 layers based on opposite-signed projections, \n",
    "    strictly bounded to the semantic binding window (Layers 12-20) \n",
    "    to prevent late-layer vocabulary conflicts.\n",
    "    \n",
    "    CRITICAL: Layers 21+ cause refutation behavior (\"not X\", \"fictional\").\n",
    "    Must stay within semantic binding window for context adoption.\n",
    "    \"\"\"\n",
    "    layer_scores = []\n",
    "    \n",
    "    # CRITICAL FIX: Bound the search strictly to mid-layers (12-20)\n",
    "    # Layers < 12: Syntax/grammar (too early)\n",
    "    # Layers > 20: Vocabulary/decision (too late, causes refutation)\n",
    "    valid_layers = [l for l in mu_pos_dict.keys() if 12 <= l <= 20]\n",
    "    \n",
    "    for layer_idx in valid_layers:\n",
    "        # Move to GPU/FP32 for safe math (stored on CPU to save VRAM)\n",
    "        mu_pos = mu_pos_dict[layer_idx].to(device=model.device, dtype=torch.float32)\n",
    "        mu_neg = mu_neg_dict[layer_idx].to(device=model.device, dtype=torch.float32)\n",
    "        \n",
    "        # Compute discriminative feature direction\n",
    "        diff = mu_pos - mu_neg\n",
    "        d_feat = diff / (torch.norm(diff, p=2) + 1e-8)\n",
    "        \n",
    "        # Uncentered projections\n",
    "        proj_pos = torch.dot(mu_pos, d_feat).item()\n",
    "        proj_neg = torch.dot(mu_neg, d_feat).item()\n",
    "        \n",
    "        score = proj_pos * proj_neg\n",
    "        \n",
    "        # Only keep layers with opposite signs (negative score)\n",
    "        if score < 0:\n",
    "            layer_scores.append((layer_idx, score))\n",
    "            \n",
    "    # Sort by most negative score (strongest geometric separation)\n",
    "    layer_scores.sort(key=lambda x: x[1])\n",
    "    \n",
    "    # CRITICAL FIX: Extract only top 2 layers to prevent cumulative mode collapse\n",
    "    # Steering 18 layers simultaneously causes gibberish output like \"also also also\"\n",
    "    L_disc = [layer_idx for layer_idx, score in layer_scores[:2]]\n",
    "    \n",
    "    # Fallback to Layer 16 (center of binding window) if no separation found\n",
    "\n",
    "    return L_disc if len(L_disc) > 0 else [16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a2a49f",
   "metadata": {},
   "source": [
    "## Phase 4: Householder Norm-Preserving Rotation Hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "711ad272",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:29:26.711387Z",
     "iopub.status.busy": "2026-02-18T08:29:26.711086Z",
     "iopub.status.idle": "2026-02-18T08:29:26.724042Z",
     "shell.execute_reply": "2026-02-18T08:29:26.723483Z",
     "shell.execute_reply.started": "2026-02-18T08:29:26.711360Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_rotation_hook(steering_vector, theta_degrees=20.0):\n",
    "    \"\"\"\n",
    "    Applies Householder rotation safely by computing norms in FP32 \n",
    "    to prevent float16 overflow, NaNs, and SDPA crashes.\n",
    "    \"\"\"\n",
    "    theta = torch.tensor(theta_degrees * (3.14159 / 180.0))\n",
    "    \n",
    "    def hook(module, inputs, output):\n",
    "        # 1. Clone to prevent in-place modification crashes in SDPA\n",
    "        if isinstance(output, tuple):\n",
    "            hidden_states = output[0].clone() \n",
    "        else:\n",
    "            hidden_states = output.clone()\n",
    "            \n",
    "        h = hidden_states[:, -1, :]\n",
    "        v = steering_vector.to(h.device)\n",
    "        \n",
    "        # 2. CRITICAL: Cast to float32 before computing L2 norms to prevent NaN overflow\n",
    "        h_fp32 = h.to(torch.float32)\n",
    "        v_fp32 = v.to(torch.float32)\n",
    "        \n",
    "        # 3. Math in FP32\n",
    "        norm_h = torch.norm(h_fp32, p=2, dim=-1, keepdim=True)\n",
    "        b_h = h_fp32 / (norm_h + 1e-8)\n",
    "        \n",
    "        v_proj_on_h = torch.sum(v_fp32 * b_h, dim=-1, keepdim=True) * b_h\n",
    "        v_ortho = v_fp32 - v_proj_on_h\n",
    "        b_2 = v_ortho / (torch.norm(v_ortho, p=2, dim=-1, keepdim=True) + 1e-8)\n",
    "        \n",
    "        h_steered_fp32 = norm_h * (torch.cos(theta) * b_h + torch.sin(theta) * b_2)\n",
    "        \n",
    "        # 4. Cast back to original dtype (fp16/bf16) and inject safely\n",
    "        hidden_states[:, -1, :] = h_steered_fp32.to(h.dtype)\n",
    "        \n",
    "        if isinstance(output, tuple):\n",
    "            return (hidden_states,) + output[1:]\n",
    "        return hidden_states\n",
    "        \n",
    "    return hook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697cfd0d",
   "metadata": {},
   "source": [
    "## Phase 3: B-PLIS with CMA-ES\n",
    "\n",
    "This section implements the Budgeted Latent Space optimizer that synthesizes unique steering vectors per query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ef12aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:29:26.725230Z",
     "iopub.status.busy": "2026-02-18T08:29:26.724931Z",
     "iopub.status.idle": "2026-02-18T08:29:27.156219Z",
     "shell.execute_reply": "2026-02-18T08:29:27.155426Z",
     "shell.execute_reply.started": "2026-02-18T08:29:26.725207Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BudgetedLatentSearch:\n",
    "    \"\"\"\n",
    "    Synthesize per-query unique steering vectors using CMA-ES in low-dimensional space.\n",
    "    CPU (NumPy FP64) to GPU (Torch FP16) bridge for compatibility.\n",
    "    \n",
    "    CRITICAL FIX: Zero-shot extraction prevents label leakage from test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, hidden_dim: int = 3072, intrinsic_dim: int = 32):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = model.device\n",
    "        self.dtype = torch.float16\n",
    "        \n",
    "        # Orthogonal projection matrix U via QR decomposition\n",
    "        U_raw = torch.randn(hidden_dim, intrinsic_dim, device=self.device, dtype=torch.float32)\n",
    "        q, _ = torch.linalg.qr(U_raw)\n",
    "        self.U = q.to(self.dtype)\n",
    "    \n",
    "    def extract_target_from_context(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Zero-shot extraction of the target entity from the context alone.\n",
    "        This guarantees zero data leakage from the test set labels.\n",
    "        \n",
    "        The optimizer must figure out the correct direction using only the provided context,\n",
    "        never peeking at the ground-truth answer from the evaluation dataset.\n",
    "        \"\"\"\n",
    "        extraction_prompt = f\"Given this context: '{context}', answer this question in 1 to 3 words ONLY: {query}\"\n",
    "        inputs = self.tokenizer(extraction_prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output_ids = self.model.generate(**inputs, max_new_tokens=5, do_sample=False, pad_token_id=self.tokenizer.eos_token_id)\n",
    "        \n",
    "        input_length = inputs.input_ids.shape[1]\n",
    "        extracted_text = self.tokenizer.decode(output_ids[0, input_length:], skip_special_tokens=True).strip()\n",
    "        return extracted_text if extracted_text else \"answer\"\n",
    "    \n",
    "    def search(self, query: str, context: str, stubborn_ans: str, \n",
    "               target_layers: List[int], budget: int = 6) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Optimize steering vector in intrinsic space using CMA-ES.\n",
    "        Reward = P(extracted_target_token) - P(stubborn_token)\n",
    "        \n",
    "        CRITICAL: No label leakage - target is extracted from context, not from test set labels.\n",
    "        \"\"\"\n",
    "        # STEP 1: Extract target from context (zero-shot, no cheating)\n",
    "        extracted_target = self.extract_target_from_context(query, context)\n",
    "        \n",
    "        # Initialize CMA-ES\n",
    "        es = cma.CMAEvolutionStrategy(\n",
    "            np.zeros(self.U.shape[1]), \n",
    "            0.5, \n",
    "            {'maxiter': budget, 'popsize': 4, 'verbose': -9}\n",
    "        )\n",
    "        \n",
    "        best_z_tensor = None\n",
    "        best_score = float('-inf')\n",
    "        \n",
    "        # Prepare prompt\n",
    "        prompt = f\"Context: {context}\\nQuestion: {query}\\nAnswer:\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors='pt').to(self.device)\n",
    "        \n",
    "        # STEP 2: Encode the extracted target (Fair game - derived from context, not labels)\n",
    "        # Extract ONLY the first token ID to avoid multi-token crashes\n",
    "        try:\n",
    "            target_tokens = self.tokenizer.encode(extracted_target, add_special_tokens=False)\n",
    "            # Ensure we get a list and extract first element\n",
    "            if isinstance(target_tokens, list) and len(target_tokens) > 0:\n",
    "                target_token_id = target_tokens[0]\n",
    "            else:\n",
    "                target_token_id = 0\n",
    "        except Exception as e:\n",
    "            target_token_id = 0\n",
    "            \n",
    "        try:\n",
    "            stubborn_tokens = self.tokenizer.encode(stubborn_ans, add_special_tokens=False)\n",
    "            if isinstance(stubborn_tokens, list) and len(stubborn_tokens) > 0:\n",
    "                stubborn_token_id = stubborn_tokens[0]\n",
    "            else:\n",
    "                stubborn_token_id = 1\n",
    "        except Exception as e:\n",
    "            stubborn_token_id = 1\n",
    "        \n",
    "        while not es.stop():\n",
    "            solutions_cpu = es.ask()\n",
    "            fitness_scores = []\n",
    "            \n",
    "            for z_cpu in solutions_cpu:\n",
    "                # CPU -> GPU bridge\n",
    "                z_tensor = torch.tensor(z_cpu, device=self.device, dtype=self.dtype)\n",
    "                delta_h = torch.matmul(self.U, z_tensor)\n",
    "                \n",
    "                # Register hooks for THIS candidate vector\n",
    "                handles = []\n",
    "                hook_fn = get_rotation_hook(delta_h, theta_degrees=20.0)\n",
    "                for l in target_layers:\n",
    "                    handles.append(self.model.model.layers[l].register_forward_hook(hook_fn))\n",
    "                \n",
    "                # Evaluate this candidate\n",
    "                try:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = self.model(**inputs)\n",
    "                        next_token_logits = outputs.logits[0, -1, :]\n",
    "                        probs = F.softmax(next_token_logits, dim=-1)\n",
    "                        \n",
    "                        # Reward: Maximize first token of extracted target, penalize parametric answer\n",
    "                        score = probs[target_token_id].item() - probs[stubborn_token_id].item()\n",
    "                except Exception as e:\n",
    "                    # If forward pass fails, assign bad score\n",
    "                    score = -1.0\n",
    "                \n",
    "                # Cleanup hooks for this candidate\n",
    "                for h in handles:\n",
    "                    h.remove()\n",
    "                \n",
    "                # CMA-ES minimizes, so negate score\n",
    "                fitness_scores.append(-score)\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_z_tensor = z_tensor\n",
    "            \n",
    "            es.tell(solutions_cpu, fitness_scores)\n",
    "        \n",
    "        # Return optimized vector in full space\n",
    "        # Fallback to zero vector if optimization failed\n",
    "        if best_z_tensor is None:\n",
    "            best_z_tensor = torch.zeros(self.U.shape[1], device=self.device, dtype=self.dtype)\n",
    "            \n",
    "        return torch.matmul(self.U, best_z_tensor)            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2b44ca",
   "metadata": {},
   "source": [
    "## Dynamic Steering Pipeline (4-Phase)\n",
    "\n",
    "Complete implementation of the spec.txt pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2331dedd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:29:27.159045Z",
     "iopub.status.busy": "2026-02-18T08:29:27.158327Z",
     "iopub.status.idle": "2026-02-18T08:29:27.171784Z",
     "shell.execute_reply": "2026-02-18T08:29:27.171227Z",
     "shell.execute_reply.started": "2026-02-18T08:29:27.159021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def dynamic_pipeline(sample: dict, mu_pos_dict: dict, mu_neg_dict: dict) -> Tuple[str, dict]:\n",
    "    \"\"\"\n",
    "    Full 4-phase dynamic steering pipeline.\n",
    "    Returns: (answer, debug_info)\n",
    "    \n",
    "    CRITICAL FIX #2: Ensure optimal vector from CMA-ES is applied to final generation\n",
    "    \"\"\"\n",
    "    query = sample['question']\n",
    "    context = sample['context']\n",
    "    # Use correct ConFiQA field names\n",
    "    correct_ans = sample.get('substituted_answer', '')  # Context-faithful answer\n",
    "    wrong_ans = sample.get('original_answer', '')  # Parametric memory answer\n",
    "    \n",
    "    debug_info = {}\n",
    "    \n",
    "    # Phase 1: Conflict Detection\n",
    "    has_conflict = detect_conflict(query, context, threshold=0.4)\n",
    "    debug_info['conflict_detected'] = has_conflict\n",
    "    \n",
    "    if not has_conflict:\n",
    "        # No steering needed\n",
    "        prompt = f\"Context: {context}\\nQuestion: {query}\\nAnswer:\"\n",
    "        inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=64,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        answer = response.split('Answer:')[-1].strip()\n",
    "        debug_info['layers_selected'] = []\n",
    "        return answer, debug_info\n",
    "    \n",
    "    # Phase 2: Discriminative Layer Selection\n",
    "    target_layers = get_discriminative_layers(mu_pos_dict, mu_neg_dict)\n",
    "    debug_info['layers_selected'] = target_layers\n",
    "    \n",
    "    # Phase 3: B-PLIS Vector Synthesis\n",
    "    # CMA-ES finds the OPTIMAL steering vector for this specific query\n",
    "    # CRITICAL: No label leakage - optimizer extracts target from context internally\n",
    "    optimizer = BudgetedLatentSearch(model, tokenizer, hidden_dim=HIDDEN_DIM, intrinsic_dim=32)\n",
    "    \n",
    "    # Handle empty answers gracefully\n",
    "    if not wrong_ans:\n",
    "        wrong_ans = \"unknown\"\n",
    "        \n",
    "    # Get the optimal vector from CMA-ES optimization (NO target_ans passed = no cheating)\n",
    "    steering_vector = optimizer.search(\n",
    "        query, context, wrong_ans, \n",
    "        target_layers, budget=6\n",
    "    )\n",
    "    \n",
    "    # Phase 4: Apply with Householder Rotation (norm-preserving for dynamic only)\n",
    "    # CRITICAL: Must register hook with the FINAL optimized vector before generation!\n",
    "    handles = []\n",
    "    hook_fn = get_rotation_hook(steering_vector, theta_degrees=20.0)\n",
    "    for l in target_layers:\n",
    "        handles.append(model.model.layers[l].register_forward_hook(hook_fn))\n",
    "    \n",
    "    # Generate with the steered model\n",
    "    prompt = f\"Context: {context}\\nQuestion: {query}\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=64,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # CRITICAL: Remove hooks immediately after generation\n",
    "    # Prevents contamination of next query\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = response.split('Answer:')[-1].strip()\n",
    "    \n",
    "    return answer, debug_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d03fc4e",
   "metadata": {},
   "source": [
    "## Verification: Check Calibration Quality\n",
    "\n",
    "Verify that offline calibration produced valid μ_pos and μ_neg vectors before evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "010541d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:29:27.172833Z",
     "iopub.status.busy": "2026-02-18T08:29:27.172590Z",
     "iopub.status.idle": "2026-02-18T08:29:27.198260Z",
     "shell.execute_reply": "2026-02-18T08:29:27.197767Z",
     "shell.execute_reply.started": "2026-02-18T08:29:27.172814Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CALIBRATION VERIFICATION\n",
      "======================================================================\n",
      "✓ Total layers calibrated: 28\n",
      "✓ μ_pos storage: torch.float32 on cpu\n",
      "✓ μ_neg storage: torch.float32 on cpu\n",
      "\n",
      "Sample Layer 0:\n",
      "  μ_pos shape: torch.Size([3072])\n",
      "  μ_pos norm: 55.0710\n",
      "  μ_neg norm: 55.3624\n",
      "\n",
      "Layer 18 (Semantic Peak):\n",
      "  proj_pos: 55.8565\n",
      "  proj_neg: -56.2095\n",
      "  score (proj_pos × proj_neg): -3139.6703\n",
      "  ✓ Opposite-signed projections detected (eligible for steering)\n",
      "======================================================================\n",
      "\n",
      "✓ Calibration complete - ready for ConFiQA evaluation\n"
     ]
    }
   ],
   "source": [
    "# Verify calibration quality\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print('CALIBRATION VERIFICATION')\n",
    "print(f'{\"=\"*70}')\n",
    "\n",
    "print(f'✓ Total layers calibrated: {len(mu_pos_dict)}')\n",
    "print(f'✓ μ_pos storage: {mu_pos_dict[0].dtype} on {mu_pos_dict[0].device}')\n",
    "print(f'✓ μ_neg storage: {mu_neg_dict[0].dtype} on {mu_neg_dict[0].device}')\n",
    "print(f'\\nSample Layer 0:')\n",
    "print(f'  μ_pos shape: {mu_pos_dict[0].shape}')\n",
    "print(f'  μ_pos norm: {torch.norm(mu_pos_dict[0]).item():.4f}')\n",
    "print(f'  μ_neg norm: {torch.norm(mu_neg_dict[0]).item():.4f}')\n",
    "\n",
    "# Check geometric separation at layer 18 (expected semantic peak)\n",
    "if 18 in mu_pos_dict:\n",
    "    mu_pos_18 = mu_pos_dict[18].to(torch.float32)\n",
    "    mu_neg_18 = mu_neg_dict[18].to(torch.float32)\n",
    "    diff_18 = mu_pos_18 - mu_neg_18\n",
    "    d_feat_18 = diff_18 / (torch.norm(diff_18) + 1e-8)\n",
    "    \n",
    "    proj_pos_18 = torch.dot(mu_pos_18, d_feat_18).item()\n",
    "    proj_neg_18 = torch.dot(mu_neg_18, d_feat_18).item()\n",
    "    \n",
    "    print(f'\\nLayer 18 (Semantic Peak):')\n",
    "    print(f'  proj_pos: {proj_pos_18:.4f}')\n",
    "    print(f'  proj_neg: {proj_neg_18:.4f}')\n",
    "    print(f'  score (proj_pos × proj_neg): {proj_pos_18 * proj_neg_18:.4f}')\n",
    "    \n",
    "    if proj_pos_18 * proj_neg_18 < 0:\n",
    "        print(f'  ✓ Opposite-signed projections detected (eligible for steering)')\n",
    "    else:\n",
    "        print(f'  ⚠️ Same-signed projections (not ideal for steering)')\n",
    "\n",
    "print(f'{\"=\"*70}\\n')\n",
    "print('✓ Calibration complete - ready for ConFiQA evaluation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5742493e",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb104974",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:29:27.199770Z",
     "iopub.status.busy": "2026-02-18T08:29:27.199491Z",
     "iopub.status.idle": "2026-02-18T08:29:27.206167Z",
     "shell.execute_reply": "2026-02-18T08:29:27.205420Z",
     "shell.execute_reply.started": "2026-02-18T08:29:27.199744Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalize_answer(text: str) -> str:\n",
    "    \"\"\"Normalize text for answer matching\"\"\"\n",
    "    import re\n",
    "    import string\n",
    "    text = str(text).lower()\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def contains_answer(generated: str, answer: str) -> bool:\n",
    "    \"\"\"Check if generated text contains the answer\"\"\"\n",
    "    if not answer:\n",
    "        return False\n",
    "    return normalize_answer(answer) in normalize_answer(generated)\n",
    "\n",
    "\n",
    "def evaluate_sample(sample: dict, mu_pos_dict: dict, mu_neg_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate one sample with dynamic steering only.\n",
    "    Returns metrics for the dynamic pipeline.\n",
    "    \"\"\"\n",
    "    substituted_answer = sample.get('substituted_answer', '').strip()\n",
    "    original_answer = sample.get('original_answer', '').strip()\n",
    "    \n",
    "    # Dynamic steering only\n",
    "    dynamic_ans, debug_info = dynamic_pipeline(sample, mu_pos_dict, mu_neg_dict)\n",
    "    dynamic_ans = dynamic_ans.strip()\n",
    "    dynamic_ps = contains_answer(dynamic_ans, substituted_answer)\n",
    "    dynamic_po = contains_answer(dynamic_ans, original_answer)\n",
    "    \n",
    "    return {\n",
    "        'question': sample['question'],\n",
    "        'substituted_answer': substituted_answer,\n",
    "        'original_answer': original_answer,\n",
    "        'answer': dynamic_ans,\n",
    "        'ps': dynamic_ps,\n",
    "        'po': dynamic_po,\n",
    "        'debug': debug_info\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92899a3f",
   "metadata": {},
   "source": [
    "## Dynamic Steering Evaluation\n",
    "\n",
    "Evaluate the 4-phase dynamic steering pipeline on ConFiQA:\n",
    "- JS divergence gating (only steer when needed)\n",
    "- Opposite-signed projection layer selection\n",
    "- CMA-ES per-query optimization\n",
    "- Householder rotation (norm-preserving)\n",
    "\n",
    "**Target**: High PS rate (context faithfulness) with diverse layer selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671b2846",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T08:29:27.207535Z",
     "iopub.status.busy": "2026-02-18T08:29:27.207273Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EVALUATING DYNAMIC STEERING (v6-Compatible Metrics)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "SUBSET: QA\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/200] PS=1.0000 PO=0.3000\n",
      "[40/200] PS=0.9000 PO=0.2250\n",
      "[60/200] PS=0.8833 PO=0.2000\n",
      "[80/200] PS=0.9000 PO=0.1875\n",
      "[120/200] PS=0.9083 PO=0.1750\n",
      "[140/200] PS=0.8786 PO=0.1714\n",
      "[160/200] PS=0.8875 PO=0.1688\n",
      "[180/200] PS=0.8833 PO=0.1611\n",
      "[200/200] PS=0.8700 PO=0.1800\n",
      "\n",
      "======================================================================\n",
      "FINAL DYNAMIC RESULTS (N=200)\n",
      "======================================================================\n",
      "  PS=0.8700  PO=0.1800  MR=0.1714  (7.16 s/ex)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "SUBSET: MR\n",
      "======================================================================\n",
      "[20/200] PS=0.9000 PO=0.3500\n",
      "[40/200] PS=0.8500 PO=0.3000\n",
      "[60/200] PS=0.8333 PO=0.3000\n",
      "[80/200] PS=0.7875 PO=0.2875\n",
      "[100/200] PS=0.7900 PO=0.3000\n",
      "[120/200] PS=0.8167 PO=0.3167\n",
      "[140/200] PS=0.8429 PO=0.3071\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import string\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Standard v6 normalization: lowercase and remove punctuation.\n",
    "    This ensures apples-to-apples comparison with baseline results.\n",
    "    \"\"\"\n",
    "    text = str(text).lower()\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUATING DYNAMIC STEERING (v6-Compatible Metrics)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize storage for visualization cells\n",
    "all_results = {}\n",
    "all_sample_results = {}\n",
    "\n",
    "# Process all three subsets\n",
    "for subset_name in ['QA', 'MR', 'MC']:\n",
    "    print(f'\\n{\"=\"*70}')\n",
    "    print(f'SUBSET: {subset_name}')\n",
    "    print(f'{\"=\"*70}')\n",
    "    \n",
    "    qa_dataset = confiqa_data[subset_name][:200]  # 200 samples per subset\n",
    "    dynamic_ps_count = 0\n",
    "    dynamic_po_count = 0\n",
    "    total_time = 0\n",
    "    sample_results = []  # Store per-sample results for visualization\n",
    "    \n",
    "    # Initialize optimizer once per subset\n",
    "    optimizer = BudgetedLatentSearch(model, tokenizer, hidden_dim=HIDDEN_DIM, intrinsic_dim=32)\n",
    "    \n",
    "    for i, sample in enumerate(qa_dataset):\n",
    "        t0 = time.perf_counter()\n",
    "        \n",
    "        query = sample['question']\n",
    "        context = sample['context']\n",
    "        target_ans = str(sample['substituted_answer'])\n",
    "        stubborn_ans = str(sample['original_answer'])\n",
    "        \n",
    "        # Step A: Gating\n",
    "        has_conflict = detect_conflict(query, context, threshold=0.4)\n",
    "        \n",
    "        if has_conflict:\n",
    "            # Step B: Layer Selection\n",
    "            L_disc = get_discriminative_layers(mu_pos_dict, mu_neg_dict)\n",
    "            \n",
    "            # Step C: Generate Vector via B-PLIS (No label leakage)\n",
    "            optimal_vector = optimizer.search(query, context, stubborn_ans, target_layers=L_disc, budget=6)\n",
    "            \n",
    "            # Step D: Apply FP32-Safe Rotation Hook\n",
    "            final_hook_fn = get_rotation_hook(optimal_vector, theta_degrees=20.0)\n",
    "            handles = []\n",
    "            for layer_idx in L_disc:\n",
    "                handles.append(model.model.layers[layer_idx].register_forward_hook(final_hook_fn))\n",
    "                \n",
    "            # Generate Steered Output\n",
    "            inputs = tokenizer(f\"Context: {context}\\nQuestion: {query}\\nAnswer:\", return_tensors=\"pt\").to(model.device)\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=64, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "            \n",
    "            # CRITICAL FIX: Isolate only the newly generated tokens (not the prompt)\n",
    "            input_length = inputs.input_ids.shape[1]\n",
    "            generated_ids = output_ids[0, input_length:]\n",
    "            generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # CRITICAL: Remove Hooks immediately to avoid contaminating the next loop\n",
    "            for handle in handles:\n",
    "                handle.remove()\n",
    "        else:\n",
    "            # No conflict detected, generate normally\n",
    "            inputs = tokenizer(f\"Context: {context}\\nQuestion: {query}\\nAnswer:\", return_tensors=\"pt\").to(model.device)\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=64, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "            \n",
    "            # CRITICAL FIX: Isolate only the newly generated tokens (not the prompt)\n",
    "            input_length = inputs.input_ids.shape[1]\n",
    "            generated_ids = output_ids[0, input_length:]\n",
    "            generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "        dt = time.perf_counter() - t0\n",
    "        total_time += dt\n",
    "        \n",
    "        # v6-Compatible Metric Calculation: Normalize and check substring match\n",
    "        gen_norm = normalize_text(generated_text)\n",
    "        target_norm = normalize_text(target_ans)\n",
    "        stubborn_norm = normalize_text(stubborn_ans)\n",
    "        \n",
    "        # PS: Does the generated text contain the context-faithful answer?\n",
    "        ps = 1 if target_norm in gen_norm else 0\n",
    "        # PO: Does the generated text contain the parametric answer?\n",
    "        po = 1 if stubborn_norm in gen_norm else 0\n",
    "        \n",
    "        dynamic_ps_count += ps\n",
    "        dynamic_po_count += po\n",
    "        \n",
    "        # Store sample-level results for visualization\n",
    "        sample_results.append({\n",
    "            'question': query,\n",
    "            'answer': generated_text,\n",
    "            'ps': ps,\n",
    "            'po': po,\n",
    "            'debug': {'layers_selected': L_disc if has_conflict else []}\n",
    "        })\n",
    "            \n",
    "        if (i + 1) % 20 == 0:\n",
    "            current_ps = dynamic_ps_count / (i + 1)\n",
    "            current_po = dynamic_po_count / (i + 1)\n",
    "            print(f\"[{i + 1}/{len(qa_dataset)}] PS={current_ps:.4f} PO={current_po:.4f}\")\n",
    "    \n",
    "    # Final v6-Compatible Metric Calculations\n",
    "    final_ps_rate = dynamic_ps_count / len(qa_dataset)\n",
    "    final_po_rate = dynamic_po_count / len(qa_dataset)\n",
    "    \n",
    "    # MR (Memorization Rate): v6 formula = PO / (PS + PO)\n",
    "    if (final_ps_rate + final_po_rate) > 0:\n",
    "        final_mr = final_po_rate / (final_ps_rate + final_po_rate)\n",
    "    else:\n",
    "        final_mr = 0.0\n",
    "    \n",
    "    avg_time = total_time / len(qa_dataset)\n",
    "    \n",
    "    # Store results for visualization cells\n",
    "    all_results[subset_name] = {\n",
    "        'ps_rate': final_ps_rate,\n",
    "        'po_rate': final_po_rate,\n",
    "        'mr': final_mr,\n",
    "        'avg_time': avg_time\n",
    "    }\n",
    "    all_sample_results[subset_name] = sample_results\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FINAL DYNAMIC RESULTS (N={len(qa_dataset)})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  PS={final_ps_rate:.4f}  PO={final_po_rate:.4f}  MR={final_mr:.4f}  ({avg_time:.2f} s/ex)\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print('EVALUATION COMPLETE')\n",
    "print(f'{\"=\"*70}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117d05fe",
   "metadata": {},
   "source": [
    "## Visualization & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0623116c",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualize dynamic steering results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "subsets = ['QA', 'MR', 'MC']\n",
    "x = np.arange(len(subsets))\n",
    "width = 0.35\n",
    "\n",
    "# PS Rate\n",
    "ax = axes[0]\n",
    "ps_vals = [all_results[s]['ps_rate'] for s in subsets]\n",
    "bars = ax.bar(x, ps_vals, width, label='PS Rate', alpha=0.8, color='#2ca02c', edgecolor='black', linewidth=1.5)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height, f'{height:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "ax.set_ylabel('PS Rate (Context Faithfulness)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Dynamic Steering: Context Faithfulness', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(subsets, fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "ax.set_ylim(0, 1.0)\n",
    "\n",
    "# PO Rate\n",
    "ax = axes[1]\n",
    "po_vals = [all_results[s]['po_rate'] for s in subsets]\n",
    "bars = ax.bar(x, po_vals, width, label='PO Rate', alpha=0.8, color='#d62728', edgecolor='black', linewidth=1.5)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height, f'{height:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "ax.set_ylabel('PO Rate (Memory Stubbornness)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Dynamic Steering: Memory Retention', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(subsets, fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "ax.set_ylim(0, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dynamic_steering_results.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n✅ Plot saved: dynamic_steering_results.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809cf562",
   "metadata": {},
   "source": [
    "## Layer Selection Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e914875",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Analyze which layers were selected by dynamic method\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print('DYNAMIC LAYER SELECTION ANALYSIS')\n",
    "print(f'{\"=\"*70}')\n",
    "\n",
    "for subset_name in ['QA', 'MR', 'MC']:\n",
    "    results = all_sample_results[subset_name]\n",
    "    chosen_layers = []\n",
    "    \n",
    "    for r in results:\n",
    "        if 'debug' in r and 'layers_selected' in r['debug']:\n",
    "            layers = r['debug']['layers_selected']\n",
    "            if isinstance(layers, list):\n",
    "                chosen_layers.extend(layers)\n",
    "            else:\n",
    "                chosen_layers.append(layers)\n",
    "    \n",
    "    if chosen_layers:\n",
    "        print(f'\\n{subset_name}:')\n",
    "        print(f'  Total layer selections: {len(chosen_layers)}')\n",
    "        print(f'  Mean layer: {np.mean(chosen_layers):.2f}')\n",
    "        print(f'  Std layer: {np.std(chosen_layers):.2f}')\n",
    "        print(f'  Layer range: [{min(chosen_layers)}, {max(chosen_layers)}]')\n",
    "        print(f'  Unique layers used: {len(set(chosen_layers))}')\n",
    "        \n",
    "        # Most common layers\n",
    "        from collections import Counter\n",
    "        layer_counts = Counter(chosen_layers)\n",
    "        top_3 = layer_counts.most_common(3)\n",
    "        print(f'  Top 3 layers: {top_3}')\n",
    "        \n",
    "        # Visualize layer distribution\n",
    "        if len(layer_counts) > 0:\n",
    "            layers_sorted = sorted(layer_counts.keys())\n",
    "            counts = [layer_counts[l] for l in layers_sorted]\n",
    "            \n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.bar(layers_sorted, counts, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "            plt.axvline(x=18, color='red', linestyle='--', linewidth=2, label='Static Best (L18)')\n",
    "            plt.xlabel('Layer', fontsize=11)\n",
    "            plt.ylabel('Selection Count', fontsize=11)\n",
    "            plt.title(f'{subset_name}: Dynamic Layer Selection Distribution', fontsize=13, fontweight='bold')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3, axis='y')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "print(f'\\n{\"=\"*70}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75d8417",
   "metadata": {},
   "source": [
    "## Interactive Demo\n",
    "\n",
    "Test the dynamic steering on custom examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e1b4aa",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Interactive demo with custom sample\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print('INTERACTIVE DEMO: BASELINE vs DYNAMIC STEERING')\n",
    "print(f'{\"=\"*70}')\n",
    "\n",
    "demo_sample = {\n",
    "    'question': 'Who was the president of the United States in 2016? give only 1 line answer and nothing else! 10 words',\n",
    "    'context': 'John Smith was the president of the United States in 2016. Jhon smith was elected to be the president of United States in 2016',\n",
    "    'substituted_answer': 'John Smith',\n",
    "    'original_answer': 'Barack Obama',\n",
    "}\n",
    "\n",
    "print(f'\\nQuestion: {demo_sample[\"question\"]}')\n",
    "print(f'Context:  {demo_sample[\"context\"]}\\n')\n",
    "print(f'Context answer: {demo_sample[\"substituted_answer\"]} (what we want)')\n",
    "print(f'Memory answer:  {demo_sample[\"original_answer\"]} (stubborn)\\n')\n",
    "\n",
    "# BASELINE INFERENCE (No Steering)\n",
    "print(f'{\"=\"*70}')\n",
    "print('BASELINE (No Steering)')\n",
    "print(f'{\"=\"*70}')\n",
    "baseline_prompt = f\"Context: {demo_sample['context']}\\nQuestion: {demo_sample['question']}\\nAnswer:\"\n",
    "baseline_inputs = tokenizer(baseline_prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    baseline_outputs = model.generate(\n",
    "        **baseline_inputs,\n",
    "        max_new_tokens=64,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Slice to get only generated tokens (not the prompt)\n",
    "baseline_input_length = baseline_inputs.input_ids.shape[1]\n",
    "baseline_generated_ids = baseline_outputs[0, baseline_input_length:]\n",
    "baseline_answer = tokenizer.decode(baseline_generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "# Check PS and PO using v6-compatible normalization\n",
    "baseline_gen_norm = normalize_text(baseline_answer)\n",
    "target_norm = normalize_text(demo_sample['substituted_answer'])\n",
    "stubborn_norm = normalize_text(demo_sample['original_answer'])\n",
    "\n",
    "baseline_ps = 1 if target_norm in baseline_gen_norm else 0\n",
    "baseline_po = 1 if stubborn_norm in baseline_gen_norm else 0\n",
    "\n",
    "print(f'Generated: {baseline_answer}')\n",
    "print(f'PS (correct): {baseline_ps} | PO (stubborn): {baseline_po}')\n",
    "\n",
    "# DYNAMIC STEERING\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print('DYNAMIC STEERING (4-Phase Pipeline)')\n",
    "print(f'{\"=\"*70}')\n",
    "result = evaluate_sample(demo_sample, mu_pos_dict, mu_neg_dict)\n",
    "\n",
    "print(f'Generated: {result[\"answer\"]}')\n",
    "print(f'PS (correct): {result[\"ps\"]} | PO (stubborn): {result[\"po\"]}')\n",
    "print(f'Selected layers: {result[\"debug\"][\"layers_selected\"]}')\n",
    "\n",
    "# COMPARISON\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print('COMPARISON')\n",
    "print(f'{\"=\"*70}')\n",
    "print(f'{\"Method\":<20} {\"PS\":<6} {\"PO\":<6} {\"Answer\"}')\n",
    "print(f'{\"-\"*70}')\n",
    "print(f'{\"Baseline\":<20} {baseline_ps:<6} {baseline_po:<6} {baseline_answer[:40]}...')\n",
    "print(f'{\"Dynamic Steering\":<20} {result[\"ps\"]:<6} {result[\"po\"]:<6} {result[\"answer\"][:40]}...')\n",
    "print(f'{\"=\"*70}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776674e0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements the complete spec.txt 4-phase dynamic activation steering pipeline for meta-llama/Llama-3.2-3B-Instruct.\n",
    "\n",
    "**Pipeline:**\n",
    "- Phase 1: JS divergence conflict detector\n",
    "- Phase 2: Opposite-signed projection layer selector\n",
    "- Phase 3: CMA-ES latent space optimizer (32D → 3072D)\n",
    "- Phase 4: Householder norm-preserving rotation\n",
    "\n",
    "**Evaluation:**\n",
    "- 200 samples × 3 subsets = 600 total\n",
    "- Metrics: PS (context faithfulness), PO (stubbornness), MR (memory retention)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9513120,
     "sourceId": 14871702,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
